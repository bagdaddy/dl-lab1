{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DjST7MxSab7w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openimages.download import download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LT3hD7KDadb5"
   },
   "outputs": [],
   "source": [
    "data_dirs = [\"val\", \"train\", \"test\"]\n",
    "number_for_samples = 200\n",
    "classes = [\"Flower\", \"Dog\", \"Cat\"]\n",
    "classes_dict = {\"flower\": 0, \"dog\": 1, \"cat\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "-FsW7_6uader"
   },
   "outputs": [],
   "source": [
    "for data_dir in data_dirs:\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pePq489aah1w",
    "outputId": "4f0e7774-d9fe-4e84-fb1b-46d61a4f26f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25  00:17:08 INFO Downloading 3000 train images for class 'flower'\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [01:58<00:00, 25.31it/s]\n",
      "2023-04-25  00:19:07 INFO Creating 3000 train annotations (pascal) for class 'flower'\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2795.02it/s]\n",
      "2023-04-25  00:19:09 INFO Downloading 2549 train images for class 'dog'\n",
      " 99%|█████████████████████████████████████████████████████████████████████████████████████▉ | 2519/2549 [01:39<00:01, 29.84it/s]2023-04-25  00:20:50 WARNING Connection pool is full, discarding connection: open-images-dataset.s3.amazonaws.com. Connection pool size: 10\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 2549/2549 [01:41<00:00, 25.18it/s]\n",
      "2023-04-25  00:20:50 INFO Creating 2549 train annotations (pascal) for class 'dog'\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2549/2549 [00:00<00:00, 5365.11it/s]\n",
      "2023-04-25  00:20:52 INFO Downloading 3000 train images for class 'cat'\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [09:02<00:00,  5.53it/s]\n",
      "2023-04-25  00:29:55 INFO Creating 3000 train annotations (pascal) for class 'cat'\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2199.58it/s]\n",
      "2023-04-25  00:31:12 INFO Downloading 200 train images for class 'flower'\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:10<00:00, 19.57it/s]\n",
      "2023-04-25  00:31:22 INFO Creating 200 train annotations (pascal) for class 'flower'\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 249.89it/s]\n",
      "2023-04-25  00:31:23 INFO Downloading 200 train images for class 'dog'\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:08<00:00, 23.67it/s]\n",
      "2023-04-25  00:31:31 INFO Creating 200 train annotations (pascal) for class 'dog'\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 205.60it/s]\n",
      "2023-04-25  00:31:33 INFO Downloading 200 train images for class 'cat'\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:08<00:00, 24.88it/s]\n",
      "2023-04-25  00:31:41 INFO Creating 200 train annotations (pascal) for class 'cat'\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 201.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'flower': {'images_dir': 'test/flower/images',\n",
       "  'annotations_dir': 'test/flower/pascal'},\n",
       " 'dog': {'images_dir': 'test/dog/images',\n",
       "  'annotations_dir': 'test/dog/pascal'},\n",
       " 'cat': {'images_dir': 'test/cat/images',\n",
       "  'annotations_dir': 'test/cat/pascal'}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Downloading is starting...\")\n",
    "\n",
    "download_dataset(\"train\", classes, limit=3000, annotation_format=\"pascal\")\n",
    "# download_dataset(\"val\", classes, limit=200, annotation_format=\"pascal\")\n",
    "download_dataset(\"test\", classes, limit=200, annotation_format=\"pascal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/mantasbagdonas/miniconda3/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mantasbagdonas/miniconda3/envs/torch/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ethAIONZah-C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from skimage.color import gray2rgb\n",
    "import glob\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),                     \n",
    "    transforms.Normalize(                      \n",
    "    mean=[0.5, 0.5, 0.5],                \n",
    "    std=[0.5, 0.5, 0.5]                  \n",
    "    )])\n",
    "\n",
    "transform_viz = transforms.Compose([           \n",
    "    transforms.ToTensor(),                     \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tjBU10-AbM4v"
   },
   "outputs": [],
   "source": [
    "def read_img(file_name, train=True, add_dim=False):\n",
    "    img = Image.open(file_name).convert(\"RGB\")\n",
    "    if train:\n",
    "        img_t = transform_train(img)\n",
    "    else:\n",
    "        img_t = transform_viz(img)\n",
    "    if add_dim:\n",
    "        img_t = torch.unsqueeze(img_t, 0)\n",
    "    return img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pYia3b54deUF"
   },
   "outputs": [],
   "source": [
    "import glob2\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_data(folder):\n",
    "    return glob2.glob(\"{}/*/pascal/*xml\".format(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gojjHk2adeWp",
    "outputId": "8fcb4a39-855f-431d-a074-12a5fd773d8e"
   },
   "outputs": [],
   "source": [
    "def get_values(data, indexes, class_min):\n",
    "    return [data[index] for index in indexes[0:class_min]]\n",
    "\n",
    "def get_data(folder, class_counts = {'a': 1600, 'b': 1600, 'c': 1600}):\n",
    "    meta_data_files = load_meta_data(folder)\n",
    "    files = []\n",
    "    values = []\n",
    "\n",
    "    for i, meta_file in enumerate(meta_data_files):\n",
    "        tree = ET.parse(meta_file)\n",
    "        root = tree.getroot()\n",
    "        file_path = root.find('path').text\n",
    "\n",
    "        if len(root.findall('object')) == 1:\n",
    "            obj = root.findall('object')[0]\n",
    "            label = obj.find('name').text\n",
    "            values.append(classes_dict[label])\n",
    "            files.append(file_path)\n",
    "            \n",
    "    first_class = [i for i, x in enumerate(values) if x == 0]\n",
    "    second_class = [i for i, x in enumerate(values) if x == 1]\n",
    "    third_class = [i for i, x in enumerate(values) if x == 2]\n",
    "    X_balanced = get_values(files, first_class, class_counts['a']) + get_values(files, second_class, class_counts['b']) + get_values(files, third_class, class_counts['c'])\n",
    "    \n",
    "    Y_balanced = get_values(values, first_class, class_counts['a']) + get_values(values, second_class, class_counts['b']) + get_values(values, third_class, class_counts['c'])\n",
    "    \n",
    "    return X_balanced, Y_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_single = {'a': 300, 'b': 730, 'c': 700}\n",
    "counts_multi = {'a': 300, 'b': 450, 'c': 450}\n",
    "\n",
    "X_train_s, Y_train_s = get_data('train', counts_single)\n",
    "X_train_m, Y_train_m = get_data('train', counts_multi)\n",
    "\n",
    "X_val, Y_val = get_data('val', {'a': 104, 'b': 104, 'c': 104})\n",
    "X_test, y_test = get_data('test', {'a': 104, 'b': 104, 'c': 104})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Count = 300\n",
      "Class 1: Count = 730\n",
      "Class 2: Count = 700\n"
     ]
    }
   ],
   "source": [
    "class_counts = np.bincount(np.array(Y_train_s))\n",
    "\n",
    "# Print the class counts\n",
    "for i, count in enumerate(class_counts):\n",
    "    print(\"Class {}: Count = {}\".format(i, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, files, y):\n",
    "        self.y = y\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file = self.files[index]\n",
    "        img = read_img(image_file)\n",
    "        \n",
    "        y = self.y[index]\n",
    "        \n",
    "        return img, (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_s = torch.tensor(Y_train_s)\n",
    "Y_train_s = F.one_hot(Y_train_s, num_classes=3)\n",
    "\n",
    "traindataset = MultiTaskDataset(X_train_s, Y_train_s)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(traindataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_m = torch.tensor(Y_train_m)\n",
    "Y_train_m = F.one_hot(Y_train_m, num_classes=3)\n",
    "\n",
    "traindataset_m = MultiTaskDataset(X_train_m, Y_train_m)\n",
    "\n",
    "trainloader_m = torch.utils.data.DataLoader(traindataset_m, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_available = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if mps_available else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining into a multitask model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.features = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        self.model.fc_single = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 3))\n",
    "        \n",
    "        self.model.fc_multi = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 3))\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.model.fc_single.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.fc_multi.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        y1 = self.model.fc_single(x)\n",
    "        y2 = self.model.fc_multi(x)\n",
    "        \n",
    "        return torch.softmax(y1, dim=1), torch.sigmoid(y2)\n",
    "    \n",
    "    def update_grad(self, single_grad=True, multi_grad=True):\n",
    "        for param in self.model.fc_single.parameters():\n",
    "            param.requires_grad = single_grad\n",
    "        for param in self.model.fc_multi.parameters():\n",
    "            param.requires_grad = multi_grad\n",
    "        \n",
    "    def print_parameters(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'fc_single' in name or 'fc_multi' in name:\n",
    "                print(name, param)\n",
    "                \n",
    "    def load_weights(self, weights):\n",
    "        self.model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "multi_label_threshold = 0.4\n",
    "\n",
    "def evaluate_standalone(model, X_test=X_test, y_test=y_test):\n",
    "    preds_single = []\n",
    "    for (i, path) in enumerate(X_test):\n",
    "        img = read_img(path, False, True)\n",
    "        img = img.to(device)\n",
    "        with torch.inference_mode():\n",
    "            predict = model(img)\n",
    "            preds_single.append(np.argmax(predict[0].detach().tolist()))\n",
    "            \n",
    "    print(classification_report(y_test, preds_single, target_names=classes))\n",
    "\n",
    "def make_multitask_predictions(model, X_test=X_test, y_test=y_test):\n",
    "    preds_single = []\n",
    "    preds_multi = []\n",
    "    for (i, path) in enumerate(X_test):\n",
    "        img = read_img(path, False, True)\n",
    "        img = img.to(device)\n",
    "        with torch.inference_mode():\n",
    "            predict = model(img)\n",
    "            preds_single.append(np.argmax(predict[0].detach().tolist()))\n",
    "            temp_m= predict[1].detach().cpu().numpy()\n",
    "            temp_m = (temp_m > multi_label_threshold).astype(int).tolist()\n",
    "            preds_multi.append(temp_m[0])\n",
    "        \n",
    "    return preds_single, preds_multi\n",
    "\n",
    "def evaluate_and_print_progress(model, loss, epoch, duration=0):\n",
    "    pred_s, pred_m = make_multitask_predictions(model)\n",
    "    y_true_m = torch.tensor(y_test)\n",
    "    y_true_m = F.one_hot(y_true_m, num_classes=3)\n",
    "        \n",
    "    accuracy_s = accuracy_score(y_test, pred_s)\n",
    "    accuracy_m = accuracy_score(y_true_m, pred_m)\n",
    "\n",
    "    print('[epoch %d]: loss: %.3f, single acc: %.3f, multi acc: %.3f, duration: %ds' %\n",
    "          (epoch + 1, loss, accuracy_s, accuracy_m, duration))\n",
    "    \n",
    "    return (accuracy_s, accuracy_m)\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test=X_test, y_test=y_test):\n",
    "    preds_single, preds_multi = make_multitask_predictions(model, X_test, y_test)\n",
    "    \n",
    "    y_true_m = torch.tensor(y_test)\n",
    "    y_true_m = F.one_hot(y_true_m, num_classes=3)\n",
    "        \n",
    "    accuracy_s = accuracy_score(y_test, preds_single)\n",
    "    accuracy_m = accuracy_score(y_true_m, preds_multi)\n",
    "    \n",
    "    print(\"Single-label accuracy: \", accuracy_s)\n",
    "    print(classification_report(y_test, preds_single, target_names=classes))\n",
    "    print()\n",
    "    print(\"Multi-label accuracy: \", accuracy_m)\n",
    "    print(classification_report(y_true_m, preds_multi, target_names=classes))\n",
    "    \n",
    "    return (accuracy_s, accuracy_m)\n",
    "    \n",
    "    \n",
    "def optimize_task(model, loader, optimizer, loss_fn, task, epoch):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, (inputs, labels) in enumerate(loader, 0):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.type(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_s, pred_m = model(inputs)\n",
    "        if task == \"single\": \n",
    "            loss = loss_fn(pred_s, labels)\n",
    "        else:\n",
    "            loss = loss_fn(pred_m, labels)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return running_loss / i\n",
    "    \n",
    "def train_single_label(model, epochs = 10):    \n",
    "    losses = []\n",
    "    single_losses = []\n",
    "    accs = []\n",
    "    model.update_grad(single_grad=True, multi_grad=False)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        loss = optimize_task(model, trainloader, optimizer_single, criterion_single, \"single\", epoch)\n",
    "                \n",
    "        end = time.time()\n",
    "            \n",
    "        model.eval()\n",
    "        acc = evaluate_and_print_progress(model, loss, epoch, end - start)\n",
    "        model.train()\n",
    "        \n",
    "        accs.append(acc)\n",
    "        single_losses.append(loss)\n",
    "                \n",
    "    return losses, single_losses, accs\n",
    "\n",
    "def train_multi_label(model, epochs = 10):    \n",
    "    losses = []\n",
    "    multi_losses = []\n",
    "    accs = []\n",
    "    model.update_grad(single_grad=False, multi_grad=True)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        loss = optimize_task(model, trainloader_m, optimizer_multi, criterion_multi, \"multi\", epoch)\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        acc = evaluate_and_print_progress(model, loss, epoch, end - start)\n",
    "        model.train()\n",
    "        \n",
    "        accs.append(acc)\n",
    "        multi_losses.append(loss)\n",
    "                \n",
    "    return losses, multi_losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mt = MultiTaskModel().to(device)\n",
    "optimizer_single = optim.Adam(filter(lambda p: p.requires_grad, model_mt.parameters()), lr=0.002)\n",
    "optimizer_multi = optim.Adam(filter(lambda p: p.requires_grad, model_mt.parameters()), lr=0.001)\n",
    "\n",
    "criterion_multi = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion_single = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1]: loss: 1.148, single acc: 0.333, multi acc: 0.147, duration: 31s\n",
      "[epoch 2]: loss: 1.146, single acc: 0.333, multi acc: 0.163, duration: 30s\n",
      "[epoch 3]: loss: 1.146, single acc: 0.333, multi acc: 0.189, duration: 30s\n",
      "[epoch 4]: loss: 1.015, single acc: 0.644, multi acc: 0.163, duration: 30s\n",
      "[epoch 5]: loss: 0.801, single acc: 0.679, multi acc: 0.176, duration: 30s\n",
      "[epoch 6]: loss: 0.770, single acc: 0.779, multi acc: 0.192, duration: 30s\n",
      "[epoch 7]: loss: 0.766, single acc: 0.721, multi acc: 0.176, duration: 29s\n",
      "[epoch 8]: loss: 0.734, single acc: 0.772, multi acc: 0.196, duration: 32s\n",
      "[epoch 9]: loss: 0.773, single acc: 0.740, multi acc: 0.167, duration: 29s\n",
      "[epoch 10]: loss: 0.728, single acc: 0.808, multi acc: 0.131, duration: 29s\n"
     ]
    }
   ],
   "source": [
    "#train single label\n",
    "losses_s, single_losses, accs = train_single_label(model_mt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1]: loss: 0.695, single acc: 0.814, multi acc: 0.000, duration: 20s\n",
      "[epoch 2]: loss: 0.676, single acc: 0.782, multi acc: 0.343, duration: 20s\n",
      "[epoch 3]: loss: 0.659, single acc: 0.833, multi acc: 0.404, duration: 20s\n",
      "[epoch 4]: loss: 0.651, single acc: 0.817, multi acc: 0.308, duration: 20s\n",
      "[epoch 5]: loss: 0.654, single acc: 0.814, multi acc: 0.391, duration: 20s\n",
      "[epoch 6]: loss: 0.645, single acc: 0.804, multi acc: 0.346, duration: 20s\n",
      "[epoch 7]: loss: 0.637, single acc: 0.798, multi acc: 0.429, duration: 20s\n",
      "[epoch 8]: loss: 0.625, single acc: 0.817, multi acc: 0.603, duration: 20s\n",
      "[epoch 9]: loss: 0.622, single acc: 0.817, multi acc: 0.625, duration: 21s\n",
      "[epoch 10]: loss: 0.621, single acc: 0.808, multi acc: 0.494, duration: 20s\n"
     ]
    }
   ],
   "source": [
    "#train multi label\n",
    "losses_m, multi_losses, accs = train_multi_label(model_mt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-label accuracy:  0.8076923076923077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.94      0.85      0.89       104\n",
      "         Dog       0.84      0.68      0.75       104\n",
      "         Cat       0.70      0.89      0.78       104\n",
      "\n",
      "    accuracy                           0.81       312\n",
      "   macro avg       0.82      0.81      0.81       312\n",
      "weighted avg       0.82      0.81      0.81       312\n",
      "\n",
      "\n",
      "Multi-label accuracy:  0.5288461538461539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.97      0.83      0.89       104\n",
      "         Dog       0.97      0.28      0.43       104\n",
      "         Cat       0.94      0.48      0.64       104\n",
      "\n",
      "   micro avg       0.96      0.53      0.68       312\n",
      "   macro avg       0.96      0.53      0.65       312\n",
      "weighted avg       0.96      0.53      0.65       312\n",
      " samples avg       0.53      0.53      0.53       312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mantasbagdonas/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8076923076923077, 0.5288461538461539)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mt.eval()\n",
    "evaluate_model(model_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-label accuracy:  0.8237179487179487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.84      0.95      0.89       104\n",
      "         Dog       0.83      0.77      0.80       104\n",
      "         Cat       0.80      0.75      0.77       104\n",
      "\n",
      "    accuracy                           0.82       312\n",
      "   macro avg       0.82      0.82      0.82       312\n",
      "weighted avg       0.82      0.82      0.82       312\n",
      "\n",
      "\n",
      "Multi-label accuracy:  0.5673076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.99      0.69      0.81       104\n",
      "         Dog       1.00      0.23      0.38       104\n",
      "         Cat       0.84      0.78      0.81       104\n",
      "\n",
      "   micro avg       0.91      0.57      0.70       312\n",
      "   macro avg       0.94      0.57      0.66       312\n",
      "weighted avg       0.94      0.57      0.66       312\n",
      " samples avg       0.57      0.57      0.57       312\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8237179487179487, 0.5673076923076923)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mt.eval()\n",
    "evaluate_model(model_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-label accuracy:  0.8237179487179487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.84      0.95      0.89       104\n",
      "         Dog       0.83      0.77      0.80       104\n",
      "         Cat       0.80      0.75      0.77       104\n",
      "\n",
      "    accuracy                           0.82       312\n",
      "   macro avg       0.82      0.82      0.82       312\n",
      "weighted avg       0.82      0.82      0.82       312\n",
      "\n",
      "\n",
      "Multi-label accuracy:  0.6185897435897436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.99      0.75      0.85       104\n",
      "         Dog       0.97      0.28      0.43       104\n",
      "         Cat       0.81      0.83      0.82       104\n",
      "\n",
      "   micro avg       0.90      0.62      0.73       312\n",
      "   macro avg       0.92      0.62      0.70       312\n",
      "weighted avg       0.92      0.62      0.70       312\n",
      " samples avg       0.62      0.62      0.62       312\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8237179487179487, 0.6185897435897436)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both at the same time:\n",
    "evaluate_model(model_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_mt.state_dict(), 'multi_task-9.0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = (np.array(single_losses) + np.array(multi_losses)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(lrs), 2, figsize=(20, 20))\n",
    "\n",
    "axs[i][0].plot(losses, c=\"r\", label=\"Loss\")\n",
    "axs[i][0].plot(all_losses_s[i], c=\"g\", label=\"Loss (s)\")\n",
    "axs[i][0].plot(all_losses_m[i], c=\"r\", label=\"Loss (m)\")\n",
    "axs[i][1].plot(all_accs[i][0], c=\"y\", label=\"Accuracy (s)\")\n",
    "axs[i][1].plot(all_accs[i][1], c=\"b\", label=\"Accuracy (m)\")\n",
    "axs[i][0].set_title('Loss progression with lr = ' + str(lrs[i]))\n",
    "axs[i][1].set_title('Accuracy progression with lr = ' + str(lrs[i]))\n",
    "axs[i][0].legend()\n",
    "axs[i][1].legend()\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out different learning rates with SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "    fig, axs = plt.subplots(len(lrs), 2, figsize=(20, 20))\n",
    "\n",
    "    for i in range(len(model_arr)):\n",
    "        axs[i][0].plot(all_losses[i], c=\"b\", label=\"Loss\")\n",
    "        axs[i][0].plot(all_losses_s[i], c=\"g\", label=\"Loss (s)\")\n",
    "        axs[i][0].plot(all_losses_m[i], c=\"r\", label=\"Loss (m)\")\n",
    "        axs[i][1].plot(all_accs[i][0], c=\"y\", label=\"Accuracy (s)\")\n",
    "        axs[i][1].plot(all_accs[i][1], c=\"b\", label=\"Accuracy (m)\")\n",
    "        axs[i][0].set_title('Loss progression with lr = ' + str(lrs[i]))\n",
    "        axs[i][1].set_title('Accuracy progression with lr = ' + str(lrs[i]))\n",
    "        axs[i][0].legend()\n",
    "        axs[i][1].legend()\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   150] loss: 0.843, single-label loss: 1.171, multi-label loss: 0.514, single acc: 0.343, multi acc: 0.333\n",
      "[1,   300] loss: 0.810, single-label loss: 1.126, multi-label loss: 0.494, single acc: 0.388, multi acc: 0.356\n",
      "[2,   150] loss: 0.705, single-label loss: 1.040, multi-label loss: 0.371, single acc: 0.590, multi acc: 0.516\n",
      "[2,   300] loss: 0.735, single-label loss: 1.071, multi-label loss: 0.398, single acc: 0.529, multi acc: 0.321\n",
      "[3,   150] loss: 0.640, single-label loss: 0.989, multi-label loss: 0.292, single acc: 0.487, multi acc: 0.455\n",
      "[3,   300] loss: 0.664, single-label loss: 1.014, multi-label loss: 0.315, single acc: 0.413, multi acc: 0.356\n",
      "[4,   150] loss: 0.604, single-label loss: 0.966, multi-label loss: 0.242, single acc: 0.452, multi acc: 0.388\n",
      "[4,   300] loss: 0.598, single-label loss: 0.956, multi-label loss: 0.241, single acc: 0.487, multi acc: 0.401\n",
      "[5,   150] loss: 0.566, single-label loss: 0.931, multi-label loss: 0.200, single acc: 0.503, multi acc: 0.212\n",
      "[5,   300] loss: 0.565, single-label loss: 0.931, multi-label loss: 0.200, single acc: 0.561, multi acc: 0.429\n",
      "[6,   150] loss: 0.520, single-label loss: 0.900, multi-label loss: 0.139, single acc: 0.510, multi acc: 0.295\n",
      "[6,   300] loss: 0.556, single-label loss: 0.926, multi-label loss: 0.187, single acc: 0.615, multi acc: 0.487\n",
      "[7,   150] loss: 0.509, single-label loss: 0.889, multi-label loss: 0.129, single acc: 0.433, multi acc: 0.375\n",
      "[7,   300] loss: 0.557, single-label loss: 0.923, multi-label loss: 0.191, single acc: 0.388, multi acc: 0.356\n",
      "[8,   150] loss: 0.498, single-label loss: 0.881, multi-label loss: 0.115, single acc: 0.407, multi acc: 0.346\n",
      "[8,   300] loss: 0.542, single-label loss: 0.916, multi-label loss: 0.168, single acc: 0.660, multi acc: 0.599\n",
      "[9,   150] loss: 0.509, single-label loss: 0.893, multi-label loss: 0.125, single acc: 0.349, multi acc: 0.337\n",
      "[9,   300] loss: 0.528, single-label loss: 0.904, multi-label loss: 0.153, single acc: 0.452, multi acc: 0.356\n",
      "[10,   150] loss: 0.502, single-label loss: 0.883, multi-label loss: 0.121, single acc: 0.561, multi acc: 0.449\n",
      "[10,   300] loss: 0.492, single-label loss: 0.882, multi-label loss: 0.103, single acc: 0.343, multi acc: 0.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   150] loss: 0.761, single-label loss: 1.142, multi-label loss: 0.380, single acc: 0.365, multi acc: 0.365\n",
      "[1,   300] loss: 0.691, single-label loss: 1.039, multi-label loss: 0.343, single acc: 0.458, multi acc: 0.369\n",
      "[2,   150] loss: 0.602, single-label loss: 0.969, multi-label loss: 0.235, single acc: 0.606, multi acc: 0.478\n",
      "[2,   300] loss: 0.595, single-label loss: 0.958, multi-label loss: 0.232, single acc: 0.580, multi acc: 0.494\n",
      "[3,   150] loss: 0.535, single-label loss: 0.912, multi-label loss: 0.158, single acc: 0.577, multi acc: 0.478\n",
      "[3,   300] loss: 0.548, single-label loss: 0.921, multi-label loss: 0.175, single acc: 0.404, multi acc: 0.378\n",
      "[4,   150] loss: 0.488, single-label loss: 0.878, multi-label loss: 0.099, single acc: 0.423, multi acc: 0.391\n",
      "[4,   300] loss: 0.510, single-label loss: 0.891, multi-label loss: 0.129, single acc: 0.433, multi acc: 0.349\n",
      "[5,   150] loss: 0.486, single-label loss: 0.879, multi-label loss: 0.094, single acc: 0.474, multi acc: 0.413\n",
      "[5,   300] loss: 0.496, single-label loss: 0.882, multi-label loss: 0.109, single acc: 0.663, multi acc: 0.535\n",
      "[6,   150] loss: 0.478, single-label loss: 0.868, multi-label loss: 0.088, single acc: 0.359, multi acc: 0.356\n",
      "[6,   300] loss: 0.494, single-label loss: 0.879, multi-label loss: 0.110, single acc: 0.625, multi acc: 0.487\n",
      "[7,   150] loss: 0.450, single-label loss: 0.848, multi-label loss: 0.051, single acc: 0.429, multi acc: 0.369\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [192]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m new_model \u001b[38;5;241m=\u001b[39m MultiTaskModel()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(new_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlrs[i], momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m losses, losses_s, losses_m, accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_multitask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(new_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel-sgd\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(lrs[i]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m model_arr\u001b[38;5;241m.\u001b[39mappend(new_model)\n",
      "Input \u001b[0;32mIn [191]\u001b[0m, in \u001b[0;36mtrain_multitask\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     51\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m criterion_single(outputs[\u001b[38;5;241m0\u001b[39m], labels)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [136]\u001b[0m, in \u001b[0;36mMultiTaskModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfc_single(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfc_multi(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m---> 96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lrs = [0.01, 0.005, 0.0005]\n",
    "\n",
    "all_losses = []\n",
    "all_losses_s = []\n",
    "all_losses_m = []\n",
    "all_accs = []\n",
    "model_arr = []\n",
    "for i in range(len(lrs)):\n",
    "    new_model = MultiTaskModel().to(device)\n",
    "    optimizer = optim.SGD(new_model.parameters(), lr=lrs[i], momentum=0.9)\n",
    "    losses, losses_s, losses_m, accs = train_multitask(new_model, optimizer, 10)\n",
    "    torch.save(new_model.state_dict(), 'model-sgd' + str(lrs[i]) + '-' + version + '.pth')\n",
    "    model_arr.append(new_model)\n",
    "    all_accs.append(accs)\n",
    "    all_losses.append(losses)\n",
    "    all_losses_s.append(losses_s)\n",
    "    all_losses_m.append(losses_m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing out different learning rates with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [0.01, 0.005, 0.0005]\n",
    "\n",
    "all_losses = []\n",
    "all_losses_s = []\n",
    "all_losses_m = []\n",
    "all_accs = []\n",
    "model_arr = []\n",
    "for i in range(len(lrs)):\n",
    "    new_model = MultiTaskModel().to(device)\n",
    "    optimizer = optim.Adam(new_model.parameters(), lr=lrs[i])\n",
    "    losses, losses_s, losses_m, accs = train_multitask(new_model, optimizer, 1)\n",
    "    torch.save(new_model.state_dict(), 'model-adam' + lrs[i] + '-' + version +  '.pth')\n",
    "    model_arr.append(new_model)\n",
    "    all_accs.append(accs)\n",
    "    all_losses.append(losses)\n",
    "    all_losses_s.append(losses_s)\n",
    "    all_losses_m.append(losses_m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(file):\n",
    "    img = read_img(file, False)\n",
    "    rgb_img = img.permute(1, 2, 0)\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ax.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(12,8))\n",
    "X_test, Y_test = get_data('test')\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        x = read_img(X_test[i + j], True, True)\n",
    "        predict = model(x)\n",
    "#         p = predict[0].detach().numpy()[0]\n",
    "#         print(p)\n",
    "        img = read_img(X_test[i + j], False)\n",
    "        rgb_img = img.permute(1, 2, 0)\n",
    "        \n",
    "        ax[i,j].imshow(rgb_img)\n",
    "        ax[i,j].set_title('actual: ' + classes[Y_true[i]] + ' predicted: ' + classes[np.argmax(p)])\n",
    "        #single label prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Multitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeMultiTaskModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeMultiTaskModel, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.features = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        self.model.fc_multi = nn.Sequential(nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            self.model.requires_grad = False\n",
    "        self.model.fc_multi.requires_grad = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return self.model.fc_multi(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = FakeMultiTaskModel()\n",
    "criterion = nn.BCEWithLogitLoss()\n",
    "optimizer_sgd = optim.SGD(model3.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.997\n",
      "[1,   200] loss: 0.786\n",
      "[1,   300] loss: 0.701\n",
      "[2,   100] loss: 0.667\n",
      "[2,   200] loss: 0.663\n",
      "[2,   300] loss: 0.651\n",
      "[3,   100] loss: 0.636\n",
      "[3,   200] loss: 0.635\n",
      "[3,   300] loss: 0.636\n",
      "[4,   100] loss: 0.619\n",
      "[4,   200] loss: 0.623\n",
      "[4,   300] loss: 0.625\n",
      "[5,   100] loss: 0.606\n",
      "[5,   200] loss: 0.605\n",
      "[5,   300] loss: 0.606\n",
      "[6,   100] loss: 0.599\n",
      "[6,   200] loss: 0.587\n",
      "[6,   300] loss: 0.588\n",
      "[7,   100] loss: 0.585\n",
      "[7,   200] loss: 0.584\n",
      "[7,   300] loss: 0.593\n",
      "[8,   100] loss: 0.578\n",
      "[8,   200] loss: 0.582\n",
      "[8,   300] loss: 0.582\n",
      "[9,   100] loss: 0.581\n",
      "[9,   200] loss: 0.584\n",
      "[9,   300] loss: 0.580\n",
      "[10,   100] loss: 0.572\n",
      "[10,   200] loss: 0.574\n",
      "[10,   300] loss: 0.577\n"
     ]
    }
   ],
   "source": [
    "losses = train_model(criterion, model3, optimizer_sgd, 10)\n",
    "torch.save(model3.state_dict(), 'wrapped-model-sgd0.001-1.0.0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.62      1.00      0.76       104\n",
      "         Dog       0.95      0.58      0.72       104\n",
      "         Cat       0.85      0.65      0.74       104\n",
      "\n",
      "    accuracy                           0.74       312\n",
      "   macro avg       0.81      0.74      0.74       312\n",
      "weighted avg       0.81      0.74      0.74       312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_standalone(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mt = MultiTaskModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('multi_task_single_label_frozen-3.0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mt.load_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-label\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.37      0.35      0.36       104\n",
      "         Dog       0.00      0.00      0.00       104\n",
      "         Cat       0.36      0.73      0.48       104\n",
      "\n",
      "    accuracy                           0.36       312\n",
      "   macro avg       0.24      0.36      0.28       312\n",
      "weighted avg       0.24      0.36      0.28       312\n",
      "\n",
      "\n",
      "Multi-label\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.45      0.24      0.31       104\n",
      "         Dog       0.33      1.00      0.50       104\n",
      "         Cat       0.33      1.00      0.50       104\n",
      "\n",
      "   micro avg       0.34      0.75      0.47       312\n",
      "   macro avg       0.37      0.75      0.44       312\n",
      "weighted avg       0.37      0.75      0.44       312\n",
      " samples avg       0.34      0.75      0.47       312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_main = MultiTaskModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultiTaskModel:\n\tsize mismatch for model.fc_multi.1.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for model.fc_multi.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for model.fc_multi.4.weight: copying a param with shape torch.Size([3, 128]) from checkpoint, the shape in current model is torch.Size([3, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [221]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m weights2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti_task_multi_label_freezed-3.0.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_main\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultiTaskModel:\n\tsize mismatch for model.fc_multi.1.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for model.fc_multi.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for model.fc_multi.4.weight: copying a param with shape torch.Size([3, 128]) from checkpoint, the shape in current model is torch.Size([3, 512])."
     ]
    }
   ],
   "source": [
    "weights2 = torch.load('multi_task_multi_label_freezed-3.0.pth')\n",
    "model_main.load_state_dict(weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-label\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.00      0.00      0.00       104\n",
      "         Dog       0.33      0.65      0.44       104\n",
      "         Cat       0.34      0.35      0.34       104\n",
      "\n",
      "    accuracy                           0.33       312\n",
      "   macro avg       0.22      0.33      0.26       312\n",
      "weighted avg       0.22      0.33      0.26       312\n",
      "\n",
      "\n",
      "Multi-label\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Flower       0.40      0.02      0.04       104\n",
      "         Dog       0.23      0.03      0.05       104\n",
      "         Cat       0.33      0.37      0.35       104\n",
      "\n",
      "   micro avg       0.33      0.14      0.19       312\n",
      "   macro avg       0.32      0.14      0.15       312\n",
      "weighted avg       0.32      0.14      0.15       312\n",
      " samples avg       0.12      0.14      0.13       312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
