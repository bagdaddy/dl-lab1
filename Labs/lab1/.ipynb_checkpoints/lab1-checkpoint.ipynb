{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DjST7MxSab7w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openimages.download import download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "id": "LT3hD7KDadb5"
   },
   "outputs": [],
   "source": [
    "data_dir = \"train\"\n",
    "number_for_samples = 2000\n",
    "classes = [\"Person\", \"Dog\", \"Cat\"]\n",
    "classes_dict = {\"person\": 0, \"dog\": 1, \"cat\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "-FsW7_6uader"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pePq489aah1w",
    "outputId": "4f0e7774-d9fe-4e84-fb1b-46d61a4f26f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11  22:46:15 INFO Downloading 2000 train images for class 'person'\n",
      "100%|██████████████████████████████████████████████████████████████████████| 2000/2000 [01:14<00:00, 27.01it/s]\n",
      "2023-04-11  22:47:29 INFO Creating 2000 train annotations (pascal) for class 'person'\n",
      "100%|████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1725.19it/s]\n",
      "2023-04-11  22:47:31 INFO Downloading 2000 train images for class 'dog'\n",
      "100%|█████████████████████████████████████████████████████████████████████▉| 1999/2000 [01:13<00:00, 36.33it/s]2023-04-11  22:48:45 WARNING Connection pool is full, discarding connection: open-images-dataset.s3.amazonaws.com. Connection pool size: 10\n",
      "100%|██████████████████████████████████████████████████████████████████████| 2000/2000 [01:13<00:00, 27.07it/s]\n",
      "2023-04-11  22:48:45 INFO Creating 2000 train annotations (pascal) for class 'dog'\n",
      "100%|████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 8186.70it/s]\n",
      "2023-04-11  22:48:47 INFO Downloading 2000 train images for class 'cat'\n",
      "100%|██████████████████████████████████████████████████████████████████████| 2000/2000 [01:12<00:00, 27.61it/s]\n",
      "2023-04-11  22:49:59 INFO Creating 2000 train annotations (pascal) for class 'cat'\n",
      "100%|████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 2269.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'person': {'images_dir': 'train/person/images',\n",
       "  'annotations_dir': 'train/person/pascal'},\n",
       " 'dog': {'images_dir': 'train/dog/images',\n",
       "  'annotations_dir': 'train/dog/pascal'},\n",
       " 'cat': {'images_dir': 'train/cat/images',\n",
       "  'annotations_dir': 'train/cat/pascal'}}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Downloading is starting...\")\n",
    "\n",
    "download_dataset(data_dir, classes, limit=number_for_samples, annotation_format=\"pascal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "0fLqkbTSah7J"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "ethAIONZah-C"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from skimage.color import gray2rgb\n",
    "import glob\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([          \n",
    "    transforms.Resize(256),                   \n",
    "    transforms.CenterCrop(224),                \n",
    "    transforms.ToTensor(),                     \n",
    "    transforms.Normalize(                      \n",
    "    mean=[0.485, 0.456, 0.406],                \n",
    "    std=[0.229, 0.224, 0.225]                  \n",
    "    )])\n",
    "\n",
    "transform_viz = transforms.Compose([           \n",
    "    transforms.ToTensor(),                     \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "tjBU10-AbM4v"
   },
   "outputs": [],
   "source": [
    "def read_img(file_name, train=True, add_dim=False):\n",
    "    img = Image.open(file_name).convert(\"RGB\")\n",
    "    if train:\n",
    "        img_t = transform(img)\n",
    "    else:\n",
    "        img_t = transform_viz(img)\n",
    "    if add_dim:\n",
    "        img_t = torch.unsqueeze(img_t, 0)\n",
    "    return img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "pYia3b54deUF"
   },
   "outputs": [],
   "source": [
    "import glob2\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_data(folder):\n",
    "    return glob2.glob(\"{}/*/pascal/*xml\".format(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gojjHk2adeWp",
    "outputId": "8fcb4a39-855f-431d-a074-12a5fd773d8e"
   },
   "outputs": [],
   "source": [
    "def get_values(data, indexes, class_min):\n",
    "    return [data[index] for index in indexes[0:class_min]]\n",
    "\n",
    "def get_data(folder):\n",
    "    meta_data_files = load_meta_data(folder)\n",
    "    files = []\n",
    "    values = []\n",
    "\n",
    "    for i, meta_file in enumerate(meta_data_files):\n",
    "        tree = ET.parse(meta_file)\n",
    "        root = tree.getroot()\n",
    "        file_path = root.find('path').text\n",
    "\n",
    "        if len(root.findall('object')) == 1:\n",
    "            obj = root.findall('object')[0]\n",
    "            label = obj.find('name').text\n",
    "            values.append(classes_dict[label])\n",
    "            files.append(file_path)\n",
    "            \n",
    "    class_counts = np.bincount(np.array(values))\n",
    "    class_min = min(class_counts)\n",
    "    \n",
    "    first_class = [i for i, x in enumerate(values) if x == 0]\n",
    "    second_class = [i for i, x in enumerate(values) if x == 1]\n",
    "    third_class = [i for i, x in enumerate(values) if x == 2]\n",
    "    X_balanced = get_values(files, first_class, class_min) + get_values(files, second_class, class_min) + get_values(files, third_class, class_min)\n",
    "    \n",
    "    Y_balanced = get_values(values, first_class, class_min) + get_values(values, second_class, class_min) + get_values(values, third_class, class_min)\n",
    "    \n",
    "    return X_balanced, Y_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Count = 1121\n",
      "Class 1: Count = 1121\n",
      "Class 2: Count = 1121\n"
     ]
    }
   ],
   "source": [
    "class_counts = np.bincount(np.array(Y_train))\n",
    "\n",
    "# Print the class counts\n",
    "for i, count in enumerate(class_counts):\n",
    "    print(\"Class {}: Count = {}\".format(i, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3363, 3]),\n",
       " tensor([[1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 1],\n",
       "         [0, 0, 1],\n",
       "         [0, 0, 1]]))"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor(Y_train)\n",
    "Y = F.one_hot(Y, num_classes=3)\n",
    "Y.shape, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "SudHAaV6bZxU"
   },
   "outputs": [],
   "source": [
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, files, y):\n",
    "        self.y = y\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file = self.files[index]\n",
    "        img = read_img(image_file)\n",
    "        \n",
    "        y = self.y[index]\n",
    "        \n",
    "        return img, (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nB_3CSYSb41W",
    "outputId": "1bf37bc9-9e1d-435a-d03e-3e8173758edc"
   },
   "outputs": [],
   "source": [
    "custom_dataset = MultiTaskDataset(X_train, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "hOxbzQvDhEyv",
    "outputId": "7a4068b3-ffb9-4770-995a-bd1cc35acd4d"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(custom_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a single-label model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model creation\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # set the number of outputs\n",
    "        self.new_number_of_cls_output = 3\n",
    "        self.new_number_of_reg_output = 3\n",
    "        \n",
    "        # our pre-trained model\n",
    "        self.model = models.resnet18(pretrained=True, requires_grad=True)\n",
    "\n",
    "        self.model.dropout = nn.Dropout(0.1)\n",
    "        self.model.fc = nn.Linear(512, self.new_number_of_cls_output)\n",
    "        \n",
    "        self.model.dropout_brach = nn.Dropout(0.5)\n",
    "        self.model.branch = nn.Linear(100352, self.new_number_of_reg_output)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)#image\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        \n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        x = self.model.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model.dropout(x)\n",
    "        \n",
    "        y_1 = self.model.fc(x) \n",
    "        y_1 = F.softmax(y_1, dim=1) #single label\n",
    "        y_2 = self.model.fc(x) \n",
    "        y_2 = F.sigmoid(y_2) #multi label\n",
    "        \n",
    "        return y_1, y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "loss_cls = nn.CrossEntropyLoss()\n",
    "loss_reg = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = []\n",
    "epoch_loss_1 = []\n",
    "epoch_loss_2 = []\n",
    "epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ep: 1] loss: 0.271, l1: 0.133, l2: 0.138\n",
      "[ep: 2] loss: 0.266, l1: 0.131, l2: 0.135\n",
      "[ep: 3] loss: 0.263, l1: 0.130, l2: 0.133\n",
      "[ep: 4] loss: 0.260, l1: 0.128, l2: 0.131\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):  \n",
    "    running_loss = 0.0\n",
    "    running_loss_1 = 0.0\n",
    "    running_loss_2 = 0.0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "\n",
    "        loss = 0\n",
    "        loss_1 = loss_cls(outputs[0].float(), labels.float())\n",
    "        loss_2 = loss_cls(outputs[1].float(), labels.float())\n",
    "        loss = loss_1 + loss_2\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_loss_1 += loss_1.item()\n",
    "        running_loss_2 += loss_2.item()\n",
    "        \n",
    "\n",
    "    print(f'[ep: {epoch + 1}] loss: {running_loss / 2000:.3f}, l1: {running_loss_1 / 2000:.3f}, l2: {running_loss_2 / 2000:.3f}')\n",
    "    epoch_loss.append(running_loss)\n",
    "    epoch_loss_1.append(running_loss_1)\n",
    "    epoch_loss_2.append(running_loss_2)\n",
    "    epochs.append(epoch)\n",
    "\n",
    "    \n",
    "\n",
    "print('Estimation done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'task1_params_cfb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"model_params_multilable.pth\" #if using [\"Clothing\", \"Mammal\", \"Bird\"]\n",
    "#model_path = \"model_params_mwf.pth\" #if using ['Man', 'Widnow', 'Flower']\n",
    "saved_state_dict = torch.load('task1_params_cfb.pth')\n",
    "model.load_state_dict(saved_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111, label=\"1\")\n",
    "\n",
    "ax.plot(epochs, epoch_loss, '-+', color=\"C0\", label='loss (ep)')\n",
    "ax.set_xlabel(\"epochs\", color=\"C0\")\n",
    "ax.set_ylim(1e-1, 1e9)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "\n",
    "ax.plot(epochs, epoch_loss_1, '-+', color=\"blue\", label='loss (ep)')\n",
    "\n",
    "ax.plot(epochs, epoch_loss_2, '-+', color=\"green\", label='loss (ep)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(12,8))\n",
    "X_test, Y_test = get_data('test')\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        x = read_img(X_test[i + j], True, True)\n",
    "        predict = model(x)\n",
    "        p = predict[0].detach().numpy()[0]\n",
    "        print(p)\n",
    "        img = read_img(X_test[i + j], False)\n",
    "        rgb_img = img.permute(1, 2, 0)\n",
    "        \n",
    "        ax[i,j].imshow(rgb_img)\n",
    "        ax[i,j].set_title('actual: ' + classes[Y_true[i]] + ' predicted: ' + classes[np.argmax(p)])\n",
    "        #single label prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(file):\n",
    "    img = read_img(file, False)\n",
    "    rgb_img = img.permute(1, 2, 0)\n",
    "    fig, ax = plt.subplots()\n",
    "    img = ax.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: Dog , actual: Person\n",
      "predicted: Cat , actual: Person\n",
      "predicted: Cat , actual: Person\n",
      "predicted: Dog , actual: Person\n",
      "predicted: Cat , actual: Person\n",
      "predicted: Dog , actual: Person\n",
      "predicted: Dog , actual: Person\n",
      "predicted: Dog , actual: Person\n",
      "predicted: Dog , actual: Person\n",
      "predicted: Dog , actual: Person\n",
      "predicted: Cat , actual: Person\n",
      "predicted: Dog , actual: Person\n",
      "predicted: Dog , actual: Dog\n",
      "predicted: Dog , actual: Dog\n",
      "predicted: Dog , actual: Dog\n",
      "predicted: Dog , actual: Dog\n",
      "predicted: Dog , actual: Dog\n",
      "predicted: Cat , actual: Dog\n",
      "predicted: Dog , actual: Dog\n",
      "predicted: Cat , actual: Dog\n",
      "predicted: Dog , actual: Dog\n",
      "predicted: Cat , actual: Dog\n",
      "predicted: Dog , actual: Dog\n",
      "predicted: Dog , actual: Dog\n",
      "predicted: Dog , actual: Cat\n",
      "predicted: Dog , actual: Cat\n",
      "predicted: Dog , actual: Cat\n",
      "predicted: Dog , actual: Cat\n",
      "predicted: Dog , actual: Cat\n",
      "predicted: Dog , actual: Cat\n",
      "predicted: Cat , actual: Cat\n",
      "predicted: Dog , actual: Cat\n",
      "predicted: Dog , actual: Cat\n",
      "predicted: Cat , actual: Cat\n",
      "predicted: Dog , actual: Cat\n",
      "predicted: Dog , actual: Cat\n"
     ]
    }
   ],
   "source": [
    "single_label_preds = []\n",
    "multi_label_preds = []\n",
    "\n",
    "for i, file in enumerate(X_test):\n",
    "    img = read_img(file, False, True)\n",
    "    predict = model(img)\n",
    "    #single label prediction\n",
    "    p = predict[0].detach().numpy()[0]\n",
    "    single_label_preds.append((p >= p[np.argmax(p)]).astype(int))\n",
    "    \n",
    "    print('predicted:', classes[np.argmax(p)], ', actual:', classes[Y_true[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
